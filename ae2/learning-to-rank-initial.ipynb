{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "terrier_path =      r\"C:\\Users\\anton\\Terrier\"\n",
    "logs_folder =       r\"C:\\Users\\anton\\Documents\\GitHub\\information-retrieval\\ae2\\logs\"\n",
    "results_folder =    r\"C:\\Users\\anton\\Documents\\GitHub\\information-retrieval\\ae2\\results\"\n",
    "features_file =     r\"C:\\Users\\anton\\Documents\\GitHub\\information-retrieval\\ae2\\features.txt\"\n",
    "terrier_index =     r\"C:\\terrier_data\\indices\\blocks_fields_stemming\"\n",
    "training_topics =   r\"C:\\terrier_data\\topics\\training\\topics\"\n",
    "validation_topics = r\"C:\\terrier_data\\topics\\validation\\topics\"\n",
    "training_qrels =    r\"C:\\terrier_data\\topics\\training\\qrels\"\n",
    "validation_qrels =  r\"C:\\terrier_data\\topics\\validation\\qrels\"\n",
    "qrels =             r\"C:\\terrier_data\\topics\\HP04\\qrels\"\n",
    "\n",
    "sh_or_bat = \"sh\"\n",
    "if os.name == 'nt':\n",
    "    sh_or_bat = \"bat\"\n",
    "\n",
    "terrier = join(terrier_path, \"bin\", \"trec_terrier.%s\" % sh_or_bat)\n",
    "teval = join(terrier_path, \"bin\", \"trec_eval.%s\" % sh_or_bat)\n",
    "letor_file= join(results_folder, \"tr.letor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(file, content, command=None):\n",
    "    with open(join(logs_folder, file), \"w\") as w:\n",
    "        if command is not None:\n",
    "            w.write(command + \"\\n\\n\")\n",
    "        for line in content:\n",
    "            w.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to retrieve results for the training topics. In this, we are going to be calculating results with multiple features, as listed in the etc/features.list file, so we use a series of Matching classes: FatFull to make a FatResultSet (i.e. a ResultSet with extra posting information), and FatFeaturedScoringMatching to add the additional features, and return a FeaturedResultSet. We then add the document label from the qrels using LabelDecorator, and write the results in a LETOR-compatible results file using Normalised2LETOROutputFormat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = join(results_folder,\"pl2_ltr.res\")\n",
    "command=\"%s -r -Dtrec.topics=%s \" % (terrier, training_topics) + \\\n",
    "    \"-Dtrec.results.file=%s -Dtrec.model=PL2 -Dterrier.index.path=%s \" % (results_file,  terrier_index) + \\\n",
    "    \"-Dtrec.matching=FatFeaturedScoringMatching,org.terrier.matching.daat.FatFull \" + \\\n",
    "    \"-Dtrec.querying.outputformat=Normalised2LETOROutputFormat \" + \\\n",
    "    \"-Dquerying.postprocesses.order=QueryExpansion,org.terrier.learning.LabelDecorator \" + \\\n",
    "    \"-Dquerying.postprocesses.controls=labels:org.terrier.learning.LabelDecorator,qe:QueryExpansion \" + \\\n",
    "    \"-Dquerying.default.controls=labels:on \" + \\\n",
    "    \"-Dlearning.labels.file=%s \" % training_qrels + \\\n",
    "    \"-Dtrec.results.file=%s -Dproximity.dependency.type=SD \" % letor_file + \\\n",
    "    \"-Dfat.featured.scoring.matching.features=FILE -Dfat.featured.scoring.matching.features.file=%s \" % features_file \n",
    "    \n",
    "run_results = !$command\n",
    "write_log(\"pl2_ltr_querying.log\", run_results, command=command)\n",
    "\n",
    "eval_file = join(results_folder, \"pl2_ltr.eval\")\n",
    "command = \"%s %s %s -q > %s\" % (teval, training_qrels, results_file, eval_file)    \n",
    "\n",
    "run_results = !$command\n",
    "write_log(\"pl2_ltr_eval.log\", run_results, command=command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets a have a look at what was output into tr.letor: \n",
    "(maybe not, it is too big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header reports the name of the features. \"score\"”\" means the model used to generate the sample, i.e. the first pass retrieval, in our case DPH. After the header, for each retrieved document for each query, there is a single line in the output. The label obtained from the qrels file is the first entry on each row.\n",
    "\n",
    "We repeat the retrieval step for the validation queries, --this time from the 2003 TREC task--:\n",
    "\n",
    "[Here](http://terrier.org/docs/v4.2/learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = join(results_folder,\"pl2.res\")\n",
    "command=\"%s -r -Dtrec.topics=%s \" % (terrier, training_topics) + \\\n",
    "    \"-Dtrec.results.file=%s -Dtrec.model=PL2 -Dterrier.index.path=%s \" % (results_file,  terrier_index)\n",
    "    \n",
    "run_results = !$command\n",
    "write_log(\"pl2_querying.log\", run_results, command=command)\n",
    "\n",
    "eval_file = join(results_folder, \"pl2.eval\")\n",
    "command = \"%s %s %s -q > %s\" % (teval, training_qrels, results_file, eval_file)    \n",
    "\n",
    "run_results = !$command\n",
    "write_log(\"pl2_eval.log\", run_results, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
