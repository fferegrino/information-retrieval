{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "terrier_path =      r\"C:\\Users\\anton\\Terrier\"\n",
    "logs_folder =       r\"C:\\Users\\anton\\Documents\\GitHub\\information-retrieval\\ae2\\logs\"\n",
    "results_folder =    r\"C:\\Users\\anton\\Documents\\GitHub\\information-retrieval\\ae2\\results\"\n",
    "features_file =     r\"C:\\Users\\anton\\Documents\\GitHub\\information-retrieval\\ae2\\features.txt\"\n",
    "terrier_index =     r\"C:\\terrier_data\\indices\\blocks_fields_stemming\"\n",
    "training_topics =   r\"C:\\terrier_data\\topics\\training\\topics\"\n",
    "validation_topics = r\"C:\\terrier_data\\topics\\validation\\topics\"\n",
    "training_qrels =    r\"C:\\terrier_data\\topics\\training\\qrels\"\n",
    "validation_qrels =  r\"C:\\terrier_data\\topics\\validation\\qrels\"\n",
    "qrels =             r\"C:\\terrier_data\\topics\\HP04\\qrels\"\n",
    "\n",
    "sh_or_bat = \"sh\"\n",
    "if os.name == 'nt':\n",
    "    sh_or_bat = \"bat\"\n",
    "\n",
    "terrier = join(terrier_path, \"bin\", \"trec_terrier.%s\" % sh_or_bat)\n",
    "teval = join(terrier_path, \"bin\", \"trec_eval.%s\" % sh_or_bat)\n",
    "letor_file= join(results_folder, \"tr.letor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(file, content, command=None):\n",
    "    with open(join(logs_folder, file), \"w\") as w:\n",
    "        if command is not None:\n",
    "            w.write(command + \"\\n\\n\")\n",
    "        for line in content:\n",
    "            w.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to retrieve results for the training topics. In this, we are going to be calculating results with multiple features, as listed in the etc/features.list file, so we use a series of Matching classes: FatFull to make a FatResultSet (i.e. a ResultSet with extra posting information), and FatFeaturedScoringMatching to add the additional features, and return a FeaturedResultSet. We then add the document label from the qrels using LabelDecorator, and write the results in a LETOR-compatible results file using Normalised2LETOROutputFormat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = join(results_folder,\"pl2_ltr.res\")\n",
    "command=\"%s -r -Dtrec.topics=%s \" % (terrier, training_topics) + \\\n",
    "    \"-Dtrec.results.file=%s -Dtrec.model=PL2 -Dterrier.index.path=%s \" % (results_file,  terrier_index) + \\\n",
    "    \"-Dtrec.matching=FatFeaturedScoringMatching,org.terrier.matching.daat.FatFull \" + \\\n",
    "    \"-Dtrec.querying.outputformat=Normalised2LETOROutputFormat \" + \\\n",
    "    \"-Dquerying.postprocesses.order=QueryExpansion,org.terrier.learning.LabelDecorator \" + \\\n",
    "    \"-Dquerying.postprocesses.controls=labels:org.terrier.learning.LabelDecorator,qe:QueryExpansion \" + \\\n",
    "    \"-Dquerying.default.controls=labels:on \" + \\\n",
    "    \"-Dlearning.labels.file=%s \" % training_qrels + \\\n",
    "    \"-Dtrec.results.file=%s -Dproximity.dependency.type=SD \" % letor_file + \\\n",
    "    \"-Dfat.featured.scoring.matching.features=FILE -Dfat.featured.scoring.matching.features.file=%s \" % features_file \n",
    "    \n",
    "run_results = !$command\n",
    "write_log(\"pl2_ltr_querying.log\", run_results, command=command)\n",
    "\n",
    "eval_file = join(results_folder, \"pl2_ltr.eval\")\n",
    "command = \"%s %s %s -q > %s\" % (teval, training_qrels, results_file, eval_file)    \n",
    "\n",
    "run_results = !$command\n",
    "write_log(\"pl2_ltr_eval.log\", run_results, command=command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets a have a look at what was output into tr.letor: \n",
    "(maybe not, it is too big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header reports the name of the features. \"score\"â€\" means the model used to generate the sample, i.e. the first pass retrieval, in our case DPH. After the header, for each retrieved document for each query, there is a single line in the output. The label obtained from the qrels file is the first entry on each row.\n",
    "\n",
    "We repeat the retrieval step for the validation queries, --this time from the 2003 TREC task--:\n",
    "\n",
    "[Here](http://terrier.org/docs/v4.2/learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = join(results_folder,\"pl2.res\")\n",
    "command=\"%s -r -Dtrec.topics=%s \" % (terrier, training_topics) + \\\n",
    "    \"-Dtrec.results.file=%s -Dtrec.model=PL2 -Dterrier.index.path=%s \" % (results_file,  terrier_index)\n",
    "    \n",
    "run_results = !$command\n",
    "write_log(\"pl2_querying.log\", run_results, command=command)\n",
    "\n",
    "eval_file = join(results_folder, \"pl2.eval\")\n",
    "command = \"%s %s %s -q > %s\" % (teval, training_qrels, results_file, eval_file)    \n",
    "\n",
    "run_results = !$command\n",
    "write_log(\"pl2_eval.log\", run_results, command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
