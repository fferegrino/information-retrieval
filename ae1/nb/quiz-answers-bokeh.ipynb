{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from bokeh.io import output_notebook, output_file, show, save\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.models import Range1d\n",
    "\n",
    "import plotting.plots as myplot\n",
    "from utils.read_eval import read_eval, get_recalls, merge_same_topic\n",
    "from utils.parse_terrier_output import parse_stats, parse_run_results, parse_evaluation_results,parse_indexing\n",
    "from utils.parse_terrier_input import read_topics\n",
    "\n",
    "tp = ['HP04', 'NP04', 'TD04']\n",
    "models = ['LogTfIdfModel', 'TF_IDF' , 'BM25', 'PL2']\n",
    "query_exp = ['','-q']\n",
    "combinations = [comb for comb in itertools.product(tp, models, query_exp)]\n",
    "\n",
    "logs_path = \"/Users/fferegrino/Documents/GitHub/ir-ae1/logs\"\n",
    "#logs_path = r\"C:\\Users\\anton\\Documents\\GitHub\\ir-ae1\\logs\"\n",
    "\n",
    "quiz_images = \"quiz_images\"\n",
    "\n",
    "topics = {}\n",
    "qrels = {}\n",
    "for topic in tp:\n",
    "    topics[topic] = r\"/Users/fferegrino/Documents/ae1/TopicsQrels/%s/topics\" % topic\n",
    "    qrels[topic] = r\"/Users/fferegrino/Documents/ae1/TopicsQrels/%s/qrels\" % topic\n",
    "\n",
    "dataframes_path= \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates = {}\n",
    "per_query = {}\n",
    "aggregates_qe = {}\n",
    "per_query_qe = {}\n",
    "for topic in tp:\n",
    "    aggregates[topic] = {}\n",
    "    per_query[topic] = {}\n",
    "    aggregates_qe[topic] = {}\n",
    "    per_query_qe[topic] = {}\n",
    "    for model in models:\n",
    "        \n",
    "        tot = pd.read_csv(os.path.join(dataframes_path, \"total_%s_%s.csv\" %(topic, model)), \n",
    "                          index_col=0)\n",
    "        pq = pd.read_csv(os.path.join(dataframes_path, \"per_query_%s_%s.csv\" %(topic, model)), \n",
    "                         index_col=0)\n",
    "        \n",
    "        tot_qe = pd.read_csv(os.path.join(dataframes_path, \"total_%s_%s-q.csv\" %(topic, model)), \n",
    "                          index_col=0)\n",
    "        pq_qe = pd.read_csv(os.path.join(dataframes_path, \"per_query_%s_%s-q.csv\" %(topic, model)), \n",
    "                         index_col=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        aggregates_qe[topic][model] = tot_qe\n",
    "        per_query_qe[topic][model] = pq_qe\n",
    "        \n",
    "        aggregates[topic][model] = tot\n",
    "        per_query[topic][model] = pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Start by using Terrier’s default indexing setup: Porter Stemming & Stopwords removed. You will need to index the collection, following the instructions in Terrier’s documentation.  \n",
    "\n",
    "**[1 mark]**\n",
    "\n",
    "Once you have indexed the collection, answer the Quiz questions asking you to enter your main obtained indexing statistics (number of tokens, size of files, time to index, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to index 1177.802 seconds\n"
     ]
    }
   ],
   "source": [
    "info_df, time = parse_indexing(os.path.join(logs_path, \"indexing-output.txt\"))\n",
    "print(\"Time to index\", time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of indexed documents: 807775\n",
      "size of vocabulary: 2043788\n",
      "number of tokens: 572916194\n",
      "number of pointers: 177737957\n",
      "time elapsed: 0.02\n"
     ]
    }
   ],
   "source": [
    "stats = parse_stats(os.path.join(logs_path, \"stats.txt\"))\n",
    "for stat in stats:\n",
    "    print(stat + \":\", stats[stat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2  \n",
    "Implement and add a new Simple TF IDF class in Terrier following the instructions in Terrier’s documentation. The Simple TF IDF weighting model you are required to implement is highlighted in Lecture 3. Use the template class provided in the IRcourseHM project, available from the [Github repo](https://github.com/cmacdonald/IRcourseHM).  \n",
    "\n",
    "**[2 marks]**  \n",
    "\n",
    "Paste your Java method code when prompted by the Quiz instance, and answer the corresponding question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "private final double constant = 0.5;\n",
    "\n",
    "public double score(double tf, double docLength) {\n",
    "\n",
    "    double score = 0;\n",
    "\n",
    "    double numerator = (numberOfDocuments - documentFrequency) + constant;\n",
    "    double denominator = documentFrequency + constant;\n",
    "\n",
    "    score = tf * (Math.log10(numerator / denominator));\n",
    "    \n",
    "    return score;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3  \n",
    "Now you will experiment with all four weighting models (Simple TF IDF, Terrier TF IDF, BM25 and PL2) and analyse their results on 3 different topic sets, representing different Web retrieval tasks: homepage finding (HP04), named page finding (NP04), and topic distillation (TD04). A description of these topic sets and the underlying search tasks is provided on Moodle.   \n",
    "\n",
    "**[16 marks]** \n",
    "\n",
    "Provide the required MAP performances of each of the weighting models over the 3 topic sets. Report your MAP performances to 4 decimal places. Also, provide the average MAP performance of each weighting model across the three topic sets, when prompted by the Quiz instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP04 - TF_IDF: 0.2089\n",
      "HP04 - BM25: 0.2186\n",
      "HP04 - PL2: 0.2251\n",
      "\n",
      "NP04 - TF_IDF: 0.4477\n",
      "NP04 - BM25: 0.4416\n",
      "NP04 - PL2: 0.4392\n",
      "\n",
      "TD04 - TF_IDF: 0.0698\n",
      "TD04 - BM25: 0.0703\n",
      "TD04 - PL2: 0.0695\n",
      "\n",
      "Average for model TF_IDF 0.2421\n",
      "Average for model BM25 0.2435\n",
      "Average for model PL2 0.2446\n"
     ]
    }
   ],
   "source": [
    "default_models = ['TF_IDF' , 'BM25', 'PL2']\n",
    "\n",
    "for t in tp:\n",
    "    avg = 0\n",
    "    for model in default_models:\n",
    "        model_avg = aggregates[t][model].iloc[0]['map']\n",
    "        avg = avg + model_avg\n",
    "        print(\"%s - %s: %.4f\" % (t, \n",
    "                                 model, \n",
    "                                 model_avg))\n",
    "    #print(\"Average for topic %.4f\" % (avg / 3))\n",
    "    print()\n",
    "\n",
    "for model in default_models:\n",
    "    avg = 0\n",
    "    for t in tp:\n",
    "        model_avg = aggregates[t][model].iloc[0]['map']\n",
    "        avg = avg + model_avg\n",
    "    print(\"Average for model %s %.4f\" % (model, avg / 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[8 marks]**  \n",
    "\n",
    "Next, for each topic set (HP04, NP04, TD04), draw a single Recall-Precision graph showing the performances of the system using each of the 4 alternative weighting models (three RecallPrecision graphs in total). Upload the resulting graphs into the Moodle instance when prompted. Then, answer the corresponding question(s) on the Quiz instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'multi_lin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9a9d60e6cb8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mlayout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'multi_lin'"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "first =None\n",
    "\n",
    "x_range = Range1d(-0.02, 1.02,bounds=(-0.02, 1.02), min_interval=0.05)\n",
    "y_range = Range1d(-0.02, 0.52,bounds=(-0.02, 0.52), min_interval=0.05)\n",
    "colors = ['#e66101', '#fdb863', '#b2abd2', '#5e3c99']\n",
    "\n",
    "for i,topic in enumerate(tp):\n",
    "    frames = [aggregates[topic][model] for model in models]\n",
    "    plot = figure(plot_width=800, plot_height=400,x_axis_label='Recall', y_axis_label='Precision')\n",
    "    plot.x_range = x_range\n",
    "    plot.y_range = y_range\n",
    "    if first is None:\n",
    "        first = plot\n",
    "    else:\n",
    "        plot.x_range = first.x_range\n",
    "        plot.y_range = first.y_range\n",
    "    \n",
    "    xs = []\n",
    "    ys = []\n",
    "    for frame, label in zip(frames, models):\n",
    "        qq = get_recalls(frame)\n",
    "        xs.append(qq.recall)\n",
    "        ys.append(qq.value)\n",
    "    plot.multi_line(xs, ys, line_color=colors)\n",
    "    plot.multi_lin(xs, ys, line_color=colors)\n",
    "    rows.append(plot)\n",
    "layout = column(rows)\n",
    "output_notebook()\n",
    "show(layout)\n",
    "output_file(os.path.join(quiz_images, \"map.html\"))\n",
    "save(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 mark]**  \n",
    "\n",
    "Finally, you should now answer on the quiz the most effective weighting model (in terms of Mean Average Precision), which you will use for the rest of Exercise 1. To identify this model, simply identify the weighting model with the highest average performance across the 3 topic sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " > Best performance was presented by the PL2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = 'PL2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "You will now conduct the Query Expansion experiments using the weighting model that produces the highest Mean Average Precision (MAP) across the 3 topic sets in Q3.  \n",
    "\n",
    "Query expansion has a few parameters, e.g. query expansion model, number of documents to analyse, number of expansion terms – you should simply use the default query expansion settings of Terrier: Bo1, 3 documents, 10 expansion terms.  \n",
    "\n",
    "**[6 marks]**\n",
    "\n",
    "First, run the best weighting model you identified in Q3 with Query Expansion on the homepage finding (HP04), named page finding (NP04), and topic distillation (TD04) topic sets. Report the obtained MAP performances in the Quiz instance. Report your MAP performances to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With query expansion HP04 - PL2: 0.2423\n",
      "With query expansion NP04 - PL2: 0.4442\n",
      "With query expansion TD04 - PL2: 0.0671\n",
      "\n",
      "Performance for model PL2 with query expansion: 0.2512\n"
     ]
    }
   ],
   "source": [
    "avg = 0\n",
    "for t in tp:\n",
    "    model_avg = aggregates_qe[t][best_model].iloc[0]['map']\n",
    "    avg = avg + model_avg\n",
    "    print(\"With query expansion %s - %s: %.4f\" % (t, \n",
    "                             best_model, \n",
    "                             model_avg))\n",
    "print()\n",
    "overall_avg = avg / 3\n",
    "print(\"Performance for model %s with query expansion: %.4f\" % (best_model, overall_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3 marks]**\n",
    "\n",
    "Next, for each topic set (HP04, NP04, TD04) draw a single Recall-Precision graph comparing the performances of your system with and without the application of Query Expansion. Upload these graphs into the Quiz instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-66dd468dd870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mp_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mp_q_qe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "for topic in tp:\n",
    "    p_q = aggregates[topic][best_model]\n",
    "    \n",
    "    p_q_qe = aggregates[topic][best_model]\n",
    "    \n",
    "    draw_comparative_recall([p_q, p_q_qe],['Without QE', 'With QE'], topic, fity=True)\n",
    "    \n",
    "    plt.savefig(os.path.join(quiz_images, \"map-query-%s-%s.png\" % (topic, best_model)), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[6 marks]**\n",
    "\n",
    "Now, for each topic set (HP04, NP04, TD04) draw a separate query-by-query histogram comparing the MAP performance of your system with and without query expansion. The histogram should show for each query of the topic set two bars: one bar corresponding to the MAP performance of the system on that given query with query expansion and one bar corresponding to the MAP performance of the system on that given query without query expansion. Using these histograms and their corresponding data, you should now be able to answer the corresponding questions of the Quiz instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for topic in tp:\n",
    "    p_q = per_query[topic][best_model]\n",
    "    p_q['type'] = \"w/o Query Expansion\"\n",
    "    p_q_qe = per_query_qe[topic][best_model]\n",
    "    p_q_qe['type'] = \"w Query Expansion\"\n",
    "\n",
    "    joint = pd.concat([p_q, p_q_qe]).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(30,10))\n",
    "    plt.title(\"Performance of %s for topics %s\" % (best_model, topic), size='x-large')\n",
    "    sns.barplot(x='query', y='map', hue='type',data=joint)\n",
    "    plt.yticks(size='large')\n",
    "    plt.xticks(size='medium')\n",
    "    plt.ylabel(\"Average performance\", size='large')\n",
    "    plt.xlabel(\"Query\", size='large')\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.12, 1.0), fontsize='large')\n",
    "    \n",
    "    plt.savefig(os.path.join(quiz_images, \"map-query-bars-%s-%s.png\" % (topic, best_model)), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
